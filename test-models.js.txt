/*
Run this model in Javascript

> npm install @azure-rest/ai-inference @azure/core-auth @azure/core-sse
*/
import ModelClient, { isUnexpected } from "@azure-rest/ai-inference";
import { AzureKeyCredential } from "@azure/core-auth";

// To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. 
// Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
const token = process.env["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"];

export async function main() {
    const client = ModelClient(
        "https://models.github.ai/inference",
        new AzureKeyCredential(token),
        {apiVersion:"2024-12-01-preview"}
    );

    const response = await client.path("/chat/completions").post({
        body: {
            messages: [
                { role: "developer", content: "You are a helpful assistant." },
                { role: "user", content: "Can you explain the basics of machine learning?" }
            ],
            model: "openai/o3"
        }
    });

    if (isUnexpected(response)) {
        throw response.body.error;
    }
    console.log(response.body.choices[0].message.content);
}

main().catch((err) => {
    console.error("The sample encountered an error:", err);
});
